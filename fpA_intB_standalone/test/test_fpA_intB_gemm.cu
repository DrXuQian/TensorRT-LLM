/*
 * Standalone test for SM80 fpA_intB GEMM (FP16 x INT4, GPTQ).
 */

#include "tensorrt_llm/common/cudaUtils.h"
#include "tensorrt_llm/kernels/cutlass_kernels/cutlass_preprocessors.h"
#include "tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/launchers/fpA_intB_gemm_sm80_wrappers.h"

#include <algorithm>
#include <cmath>
#include <cstdint>
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <limits>
#include <string>
#include <vector>

namespace
{
struct Args
{
    int m = 1;
    int n = 4096;
    int k = 4096;
    int group_size = 128;
    bool verify = false;
    bool list_configs = false;
    bool force_config = false;
    bool force_cuda = false;
    tensorrt_llm::cutlass_extensions::CutlassGemmConfig forced_config{};
};

void print_usage(char const* name)
{
    std::printf(
        "Usage: %s [--m=N] [--n=N] [--k=N] [--group_size=N] [--verify] [--list_configs] "
        "[--config=cuda|tile_m,tile_n,tile_k,stages,split_k]\n",
        name);
}

bool parse_int(char const* arg, char const* key, int& out)
{
    size_t const len = std::strlen(key);
    if (std::strncmp(arg, key, len) != 0)
    {
        return false;
    }
    out = std::strtol(arg + len, nullptr, 10);
    return true;
}

bool tile_from_shape(int tile_m, int tile_n, int tile_k,
    tensorrt_llm::cutlass_extensions::CutlassTileConfig& out)
{
    using Config = tensorrt_llm::cutlass_extensions::CutlassTileConfig;
    if (tile_k != 64)
    {
        return false;
    }
    if (tile_m == 16 && tile_n == 128)
    {
        out = Config::CtaShape16x128x64_WarpShape16x32x64;
        return true;
    }
    if (tile_m == 16 && tile_n == 256)
    {
        out = Config::CtaShape16x256x64_WarpShape16x64x64;
        return true;
    }
    if (tile_m == 32 && tile_n == 128)
    {
        out = Config::CtaShape32x128x64_WarpShape32x32x64;
        return true;
    }
    if (tile_m == 64 && tile_n == 128)
    {
        out = Config::CtaShape64x128x64_WarpShape64x32x64;
        return true;
    }
    if (tile_m == 128 && tile_n == 128)
    {
        out = Config::CtaShape128x128x64_WarpShape128x32x64;
        return true;
    }
    return false;
}

bool parse_config_spec(char const* spec, Args& args)
{
    if (spec == nullptr || *spec == '\0')
    {
        return false;
    }
    if (std::strcmp(spec, "cuda") == 0 || std::strcmp(spec, "cuda_kernel") == 0)
    {
        args.force_cuda = true;
        args.force_config = true;
        return true;
    }

    std::string normalized(spec);
    for (char& ch : normalized)
    {
        if (ch == 'x' || ch == 'X')
        {
            ch = ',';
        }
    }

    int tile_m = 0;
    int tile_n = 0;
    int tile_k = 0;
    int stages = 0;
    int split_k = 1;
    int parsed = std::sscanf(normalized.c_str(), "%d,%d,%d,%d,%d", &tile_m, &tile_n, &tile_k, &stages, &split_k);
    if (parsed < 4)
    {
        return false;
    }
    if (tile_m <= 0 || tile_n <= 0 || tile_k <= 0 || stages <= 0 || split_k <= 0)
    {
        return false;
    }

    tensorrt_llm::cutlass_extensions::CutlassTileConfig tile_config;
    if (!tile_from_shape(tile_m, tile_n, tile_k, tile_config))
    {
        return false;
    }

    args.forced_config.tile_config_sm80 = tile_config;
    args.forced_config.stages = stages;
    args.forced_config.split_k_factor = split_k;
    args.forced_config.split_k_style = split_k > 1
        ? tensorrt_llm::cutlass_extensions::SplitKStyle::SPLIT_K_SERIAL
        : tensorrt_llm::cutlass_extensions::SplitKStyle::NO_SPLIT_K;
    args.forced_config.enableCudaKernel = false;
    args.forced_config.sm_version = 80;
    args.force_config = true;
    return true;
}

void parse_args(int argc, char** argv, Args& args)
{
    for (int i = 1; i < argc; ++i)
    {
        if (std::strcmp(argv[i], "--help") == 0)
        {
            print_usage(argv[0]);
            std::exit(0);
        }
        if (parse_int(argv[i], "--m=", args.m))
        {
            continue;
        }
        if (parse_int(argv[i], "--n=", args.n))
        {
            continue;
        }
        if (parse_int(argv[i], "--k=", args.k))
        {
            continue;
        }
        if (parse_int(argv[i], "--group_size=", args.group_size))
        {
            continue;
        }
        if (std::strcmp(argv[i], "--verify") == 0)
        {
            args.verify = true;
            continue;
        }
        if (std::strcmp(argv[i], "--list_configs") == 0)
        {
            args.list_configs = true;
            continue;
        }
        if (std::strncmp(argv[i], "--config=", 9) == 0)
        {
            if (!parse_config_spec(argv[i] + 9, args))
            {
                std::fprintf(stderr,
                    "Invalid --config format. Use --config=cuda or --config=tile_m,tile_n,tile_k,stages,split_k.\n");
                print_usage(argv[0]);
                std::exit(1);
            }
            continue;
        }
        std::fprintf(stderr, "Unknown argument: %s\n", argv[i]);
        print_usage(argv[0]);
        std::exit(1);
    }
}

void print_all_configs()
{
    using Config = tensorrt_llm::cutlass_extensions::CutlassGemmConfig;
    Config const* configs = nullptr;
    size_t const count = tensorrt_llm::kernels::cutlass_kernels_oss::fpA_intB_get_all_configs(&configs);
    std::printf("Supported configs (%zu):\n", count);
    for (size_t i = 0; i < count; ++i)
    {
        Config const& cfg = configs[i];
        if (cfg.enableCudaKernel)
        {
            std::printf("  %zu: cuda_kernel\n", i);
        }
        else
        {
            std::printf("  %zu: tile_enum=%d stages=%d split_k=%d\n", i,
                static_cast<int>(cfg.tile_config_sm80), cfg.stages, cfg.split_k_factor);
        }
    }
}

bool get_cuda_config(tensorrt_llm::cutlass_extensions::CutlassGemmConfig& out)
{
    using Config = tensorrt_llm::cutlass_extensions::CutlassGemmConfig;
    Config const* configs = nullptr;
    size_t const count = tensorrt_llm::kernels::cutlass_kernels_oss::fpA_intB_get_all_configs(&configs);
    for (size_t i = 0; i < count; ++i)
    {
        if (configs[i].enableCudaKernel)
        {
            out = configs[i];
            return true;
        }
    }
    return false;
}

void quantize_groupwise_int4(std::vector<half>& scales, std::vector<half>& zeros, std::vector<int8_t>& packed_weights,
    std::vector<int8_t>& signed_weights, std::vector<half> const& weights, int k, int n, int group_size)
{
    int const groups = k / group_size;
    scales.assign(static_cast<size_t>(groups) * n, __float2half_rn(0.0f));
    zeros.assign(static_cast<size_t>(groups) * n, __float2half_rn(0.0f));
    signed_weights.assign(static_cast<size_t>(k) * n, 0);

    for (int g = 0; g < groups; ++g)
    {
        int const k_start = g * group_size;
        int const k_end = k_start + group_size;
        for (int col = 0; col < n; ++col)
        {
            float max_abs = 0.0f;
            for (int row = k_start; row < k_end; ++row)
            {
                float const val = __half2float(weights[static_cast<size_t>(row) * n + col]);
                max_abs = std::max(max_abs, std::fabs(val));
            }
            float const scale = max_abs > 0.0f ? max_abs / 7.0f : 1.0f;
            scales[static_cast<size_t>(g) * n + col] = __float2half_rn(scale);

            int const qzero = 7; // GPTQ zero-point encoding (zero point = qzero + 1)
            zeros[static_cast<size_t>(g) * n + col] = __float2half_rn((7 - qzero) * scale);

            for (int row = k_start; row < k_end; ++row)
            {
                float const val = __half2float(weights[static_cast<size_t>(row) * n + col]);
                float const scaled = scale > 0.0f ? std::round(val / scale) : 0.0f;
                int q = static_cast<int>(scaled);
                q = std::max(-8, std::min(7, q));
                signed_weights[static_cast<size_t>(row) * n + col] = static_cast<int8_t>(q);
            }
        }
    }

    int const bytes_per_row = n / 2;
    packed_weights.assign(static_cast<size_t>(k) * bytes_per_row, 0);
    for (int row = 0; row < k; ++row)
    {
        for (int byte_idx = 0; byte_idx < bytes_per_row; ++byte_idx)
        {
            int const col0 = 2 * byte_idx;
            int const col1 = col0 + 1;
            int8_t q0 = signed_weights[static_cast<size_t>(row) * n + col0];
            int8_t q1 = signed_weights[static_cast<size_t>(row) * n + col1];
            uint8_t u0 = static_cast<uint8_t>(q0) & 0x0F;
            uint8_t u1 = static_cast<uint8_t>(q1) & 0x0F;
            packed_weights[static_cast<size_t>(row) * bytes_per_row + byte_idx] = static_cast<int8_t>(u0 | (u1 << 4));
        }
    }
}

struct DiffStats
{
    float max_abs = 0.0f;
    float max_rel = 0.0f;
    size_t max_idx = 0;
};

DiffStats compare_half_to_float(std::vector<half> const& gpu, std::vector<float> const& ref)
{
    DiffStats stats;
    size_t const n = std::min(gpu.size(), ref.size());
    for (size_t i = 0; i < n; ++i)
    {
        float const gpu_val = __half2float(gpu[i]);
        float const ref_val = ref[i];
        float const abs_err = std::fabs(gpu_val - ref_val);
        float const rel_err = abs_err / (std::fabs(ref_val) + 1e-6f);
        if (abs_err > stats.max_abs)
        {
            stats.max_abs = abs_err;
            stats.max_idx = i;
        }
        if (rel_err > stats.max_rel)
        {
            stats.max_rel = rel_err;
        }
    }
    return stats;
}

} // namespace

int main(int argc, char** argv)
{
    Args args;
    parse_args(argc, argv, args);

    if (args.list_configs)
    {
        print_all_configs();
        return 0;
    }

    if (args.m <= 0 || args.n <= 0 || args.k <= 0)
    {
        std::fprintf(stderr, "m, n, k must be positive.\n");
        return 1;
    }
    if (args.group_size != 128 && args.group_size != 64)
    {
        std::fprintf(stderr, "group_size must be 64 or 128.\n");
        return 1;
    }
    if ((args.k % args.group_size) != 0)
    {
        std::fprintf(stderr, "k must be multiple of group_size.\n");
        return 1;
    }
    if ((args.k % 64) != 0)
    {
        std::fprintf(stderr, "k must be multiple of 64 for SM80 fpA_intB.\n");
        return 1;
    }
    if ((args.n % 2) != 0)
    {
        std::fprintf(stderr, "n must be even for int4 packing.\n");
        return 1;
    }

    std::printf("m=%d n=%d k=%d group_size=%d\n", args.m, args.n, args.k, args.group_size);

    std::vector<half> h_a(static_cast<size_t>(args.m) * args.k);
    std::vector<half> h_w(static_cast<size_t>(args.k) * args.n);

    for (size_t i = 0; i < h_a.size(); ++i)
    {
        h_a[i] = __float2half_rn(static_cast<float>(i % 101) * 0.01f);
    }
    for (size_t i = 0; i < h_w.size(); ++i)
    {
        h_w[i] = __float2half_rn(static_cast<float>(i % 79) * 0.02f);
    }

    std::vector<half> h_scales;
    std::vector<half> h_zeros;
    std::vector<int8_t> h_packed;
    std::vector<int8_t> h_signed;
    quantize_groupwise_int4(h_scales, h_zeros, h_packed, h_signed, h_w, args.k, args.n, args.group_size);

    std::vector<int8_t> h_preprocessed(h_packed.size(), 0);
    std::vector<size_t> shape = {static_cast<size_t>(args.k), static_cast<size_t>(args.n)};
    tensorrt_llm::kernels::cutlass_kernels::preprocess_weights_for_mixed_gemm(
        h_preprocessed.data(), h_packed.data(), shape, tensorrt_llm::kernels::cutlass_kernels::QuantType::W4_A16, true);

    half* d_a = nullptr;
    int8_t* d_b = nullptr;
    half* d_scales = nullptr;
    half* d_zeros = nullptr;
    half* d_c = nullptr;

    size_t const a_bytes = h_a.size() * sizeof(half);
    size_t const b_bytes = h_preprocessed.size();
    size_t const scale_bytes = h_scales.size() * sizeof(half);
    size_t const c_bytes = static_cast<size_t>(args.m) * args.n * sizeof(half);

    tensorrt_llm::common::check_cuda_error(cudaMalloc(&d_a, a_bytes));
    tensorrt_llm::common::check_cuda_error(cudaMalloc(&d_b, b_bytes));
    tensorrt_llm::common::check_cuda_error(cudaMalloc(&d_scales, scale_bytes));
    tensorrt_llm::common::check_cuda_error(cudaMalloc(&d_zeros, scale_bytes));
    tensorrt_llm::common::check_cuda_error(cudaMalloc(&d_c, c_bytes));

    tensorrt_llm::common::check_cuda_error(cudaMemcpy(d_a, h_a.data(), a_bytes, cudaMemcpyHostToDevice));
    tensorrt_llm::common::check_cuda_error(cudaMemcpy(d_b, h_preprocessed.data(), b_bytes, cudaMemcpyHostToDevice));
    tensorrt_llm::common::check_cuda_error(cudaMemcpy(d_scales, h_scales.data(), scale_bytes, cudaMemcpyHostToDevice));
    tensorrt_llm::common::check_cuda_error(cudaMemcpy(d_zeros, h_zeros.data(), scale_bytes, cudaMemcpyHostToDevice));

    size_t workspace_bytes = tensorrt_llm::kernels::cutlass_kernels_oss::fpA_intB_get_workspace_size(
        args.m, args.n, args.k);
    void* d_workspace = nullptr;
    if (workspace_bytes > 0)
    {
        tensorrt_llm::common::check_cuda_error(cudaMalloc(&d_workspace, workspace_bytes));
    }

    cudaStream_t stream{};
    tensorrt_llm::common::check_cuda_error(cudaStreamCreate(&stream));

    tensorrt_llm::cutlass_extensions::CutlassGemmConfig config{};
    if (args.force_config)
    {
        if (args.force_cuda)
        {
            if (!get_cuda_config(config))
            {
                std::fprintf(stderr, "CUDA kernel config not found in candidate list.\\n");
                return 1;
            }
        }
        else
        {
            config = args.forced_config;
        }
        if (!tensorrt_llm::kernels::cutlass_kernels_oss::fpA_intB_is_supported_config(config))
        {
            std::fprintf(stderr, "Forced config not supported. Use --list_configs.\n");
            return 1;
        }
    }
    else
    {
        bool ok = tensorrt_llm::kernels::cutlass_kernels_oss::fpA_intB_select_config_fp16_int4_gptq(
            config, args.m, args.n, args.k, args.group_size, 0, true);
        if (!ok)
        {
            std::fprintf(stderr, "No valid config found for the given shape.\n");
            return 1;
        }
    }

    if (config.enableCudaKernel)
    {
        std::printf("selected config: cuda_kernel\n");
    }
    else
    {
        std::printf("selected config: tile_enum=%d stages=%d split_k=%d\n", static_cast<int>(config.tile_config_sm80),
            config.stages, config.split_k_factor);
    }

    tensorrt_llm::kernels::cutlass_kernels_oss::fpA_intB_gemm_fp16_int4_gptq_with_config(
        d_a, d_b, d_scales, d_zeros, d_c, args.m, args.n, args.k, args.group_size, stream, d_workspace,
        workspace_bytes, config);

    tensorrt_llm::common::check_cuda_error(cudaStreamSynchronize(stream));

    std::vector<half> h_out(static_cast<size_t>(args.m) * args.n);
    tensorrt_llm::common::check_cuda_error(cudaMemcpy(h_out.data(), d_c, c_bytes, cudaMemcpyDeviceToHost));

    if (args.verify)
    {
        bool const small_case = args.m <= 4 && args.n <= 256 && args.k <= 256;
        if (!small_case)
        {
            std::printf("verify requested, but sizes are too large; skipping CPU reference.\n");
        }
        else
        {
            std::vector<float> ref(h_out.size(), 0.0f);
            for (int m = 0; m < args.m; ++m)
            {
                for (int n = 0; n < args.n; ++n)
                {
                    float acc = 0.0f;
                    for (int k = 0; k < args.k; ++k)
                    {
                        int const group = k / args.group_size;
                        half const scale_h = h_scales[static_cast<size_t>(group) * args.n + n];
                        half const zero_h = h_zeros[static_cast<size_t>(group) * args.n + n];
                        float const scale = __half2float(scale_h);
                        float const zero = __half2float(zero_h);

                        int q = static_cast<int>(h_signed[static_cast<size_t>(k) * args.n + n]);
                        float const w = static_cast<float>(q) * scale + zero;
                        float const a = __half2float(h_a[static_cast<size_t>(m) * args.k + k]);
                        acc += a * w;
                    }
                    ref[static_cast<size_t>(m) * args.n + n] = acc;
                }
            }

            DiffStats stats = compare_half_to_float(h_out, ref);
            float const abs_tol = 5e-2f;
            float const rel_tol = 5e-2f;
            bool const pass = (stats.max_abs <= abs_tol) || (stats.max_rel <= rel_tol);
            std::printf("verify: max_abs=%.6f max_rel=%.6f %s\n", stats.max_abs, stats.max_rel,
                pass ? "PASS" : "FAIL");
        }
    }
    else
    {
        float checksum = 0.0f;
        size_t const sample = std::min<size_t>(h_out.size(), 1024);
        for (size_t i = 0; i < sample; ++i)
        {
            checksum += __half2float(h_out[i]);
        }
        std::printf("checksum=%.6f\n", checksum);
    }

    tensorrt_llm::common::check_cuda_error(cudaFree(d_a));
    tensorrt_llm::common::check_cuda_error(cudaFree(d_b));
    tensorrt_llm::common::check_cuda_error(cudaFree(d_scales));
    tensorrt_llm::common::check_cuda_error(cudaFree(d_zeros));
    tensorrt_llm::common::check_cuda_error(cudaFree(d_c));
    if (d_workspace)
    {
        tensorrt_llm::common::check_cuda_error(cudaFree(d_workspace));
    }
    tensorrt_llm::common::check_cuda_error(cudaStreamDestroy(stream));

    return 0;
}
